{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"gpt-4-turbo\",\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"together_ai/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\",\n",
    "    \"together_ai/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
    "    \"together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    \"together_ai/mistralai/Mixtral-8x22B-Instruct-v0.1\",\n",
    "    \"together_ai/Qwen/Qwen1.5-72B-Chat\",\n",
    "    \"together_ai/Qwen/Qwen2-72B-Instruct\",\n",
    "    \"together_ai/Qwen/Qwen1.5-110B-Chat\",\n",
    "    \"together_ai/meta-llama/Meta-Llama-3-70B-Instruct-Turbo\",\n",
    "    \"together_ai/meta-llama/Meta-Llama-3-8B-Instruct-Turbo\",\n",
    "    \"together_ai/deepseek-ai/deepseek-llm-67b-chat\",\n",
    "]\n",
    "\n",
    "tags = [\n",
    "    \"benchmark_gpt-4-turbo_gpt-4o-2024-08-06_gpt-4o-2024-08-06_haicosystem_trial2\",\n",
    "    \"benchmark_gpt-3.5-turbo_gpt-4o-2024-08-06_gpt-4o-2024-08-06_haicosystem_trial2\",\n",
    "    \"benchmark_together_ai/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo_gpt-4o-2024-08-06_gpt-4o-2024-08-06_haicosystem_trial2\",\n",
    "    \"benchmark_together_ai/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo_gpt-4o-2024-08-06_gpt-4o-2024-08-06_haicosystem_trial2\",\n",
    "    \"benchmark_together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo_gpt-4o-2024-08-06_gpt-4o-2024-08-06_haicosystem_trial2\",\n",
    "    \"benchmark_together_ai/mistralai/Mixtral-8x22B-Instruct-v0.1_gpt-4o-2024-08-06_gpt-4o-2024-08-06_haicosystem_trial2\",\n",
    "    \"benchmark_together_ai/Qwen/Qwen1.5-72B-Chat_gpt-4o-2024-08-06_gpt-4o-2024-08-06_haicosystem_trial2\",\n",
    "    \"benchmark_together_ai/Qwen/Qwen2-72B-Instruct_gpt-4o-2024-08-06_gpt-4o-2024-08-06_haicosystem_trial2\",\n",
    "    \"benchmark_together_ai/Qwen/Qwen1.5-110B-Chat_gpt-4o-2024-08-06_gpt-4o-2024-08-06_haicosystem_trial2\",\n",
    "    \"benchmark_together_ai/meta-llama/Meta-Llama-3-70B-Instruct-Turbo_gpt-4o-2024-08-06_gpt-4o-2024-08-06_haicosystem_trial2\",\n",
    "    \"benchmark_together_ai/meta-llama/Meta-Llama-3-8B-Instruct-Turbo_gpt-4o-2024-08-06_gpt-4o-2024-08-06_haicosystem_trial2\",\n",
    "    \"benchmark_together_ai/deepseek-ai/deepseek-llm-67b-chat_gpt-4o-2024-08-06_gpt-4o-2024-08-06_haicosystem_trial2\",\n",
    "]\n",
    "\n",
    "models_mapping = {\n",
    "    \"gpt-4-turbo\": [\"GPT-4-turbo\", \"OpenAI\", \"No\"],\n",
    "    \"gpt-3.5-turbo\": [\"GPT-3.5-turbo\", \"OpenAI\", \"No\"],\n",
    "    \"together_ai/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\": [\n",
    "        \"Llama3.1-405B\",\n",
    "        \"Meta\",\n",
    "        \"Yes\",\n",
    "    ],\n",
    "    \"together_ai/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\": [\n",
    "        \"Llama3.1-70B\",\n",
    "        \"Meta\",\n",
    "        \"Yes\",\n",
    "    ],\n",
    "    \"together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\": [\n",
    "        \"Llama3.1-8B\",\n",
    "        \"Meta\",\n",
    "        \"Yes\",\n",
    "    ],\n",
    "    \"together_ai/mistralai/Mixtral-8x22B-Instruct-v0.1\": [\n",
    "        \"Mixtral-8x22B\",\n",
    "        \"MistralAI\",\n",
    "        \"Yes\",\n",
    "    ],\n",
    "    \"together_ai/Qwen/Qwen1.5-72B-Chat\": [\"Qwen1.5-72B-Chat\", \"Alibaba\", \"Yes\"],\n",
    "    \"together_ai/Qwen/Qwen2-72B-Instruct\": [\"Qwen2-72B-Instruct\", \"Alibaba\", \"Yes\"],\n",
    "    \"together_ai/Qwen/Qwen1.5-110B-Chat\": [\"Qwen1.5-110B-Chat\", \"Alibaba\", \"Yes\"],\n",
    "    \"together_ai/meta-llama/Meta-Llama-3-70B-Instruct-Turbo\": [\n",
    "        \"Llama3-70B\",\n",
    "        \"Meta\",\n",
    "        \"Yes\",\n",
    "    ],\n",
    "    \"together_ai/meta-llama/Meta-Llama-3-8B-Instruct-Turbo\": [\n",
    "        \"Llama3-8B\",\n",
    "        \"Meta\",\n",
    "        \"Yes\",\n",
    "    ],\n",
    "    \"together_ai/deepseek-ai/deepseek-llm-67b-chat\": [\n",
    "        \"DeepSeek-67B\",\n",
    "        \"DeepSeek AI\",\n",
    "        \"Yes\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\n",
    "    \"benchmark_gpt-4-turbo_gpt-4o-2024-08-06_gemini-gemini-2.5-flash-preview-05-20_haicosystem_trial2\",\n",
    "    \"benchmark_gpt-3.5-turbo_gpt-4o-2024-08-06_gemini-gemini-2.5-flash-preview-05-20_haicosystem_trial2\",\n",
    "    \"benchmark_together_ai/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo_gpt-4o-2024-08-06_gemini-gemini-2.5-flash-preview-05-20_haicosystem_trial2\",\n",
    "    \"benchmark_together_ai/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo_gpt-4o-2024-08-06_gemini-gemini-2.5-flash-preview-05-20_haicosystem_trial2\",\n",
    "    \"benchmark_together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo_gpt-4o-2024-08-06_gemini-gemini-2.5-flash-preview-05-20_haicosystem_trial2\",\n",
    "    \"benchmark_together_ai/mistralai/Mixtral-8x22B-Instruct-v0.1_gpt-4o-2024-08-06_gemini-gemini-2.5-flash-preview-05-20_haicosystem_trial2\",\n",
    "    \"benchmark_together_ai/Qwen/Qwen1.5-72B-Chat_gpt-4o-2024-08-06_gemini-gemini-2.5-flash-preview-05-20_haicosystem_trial2\",\n",
    "    \"benchmark_together_ai/Qwen/Qwen2-72B-Instruct_gpt-4o-2024-08-06_gemini-gemini-2.5-flash-preview-05-20_haicosystem_trial2\",\n",
    "    \"benchmark_together_ai/Qwen/Qwen1.5-110B-Chat_gpt-4o-2024-08-06_gemini-gemini-2.5-flash-preview-05-20_haicosystem_trial2\",\n",
    "    \"benchmark_together_ai/meta-llama/Meta-Llama-3-70B-Instruct-Turbo_gpt-4o-2024-08-06_gemini-gemini-2.5-flash-preview-05-20_haicosystem_trial2\",\n",
    "    \"benchmark_together_ai/meta-llama/Meta-Llama-3-8B-Instruct-Turbo_gpt-4o-2024-08-06_gemini-gemini-2.5-flash-preview-05-20_haicosystem_trial2\",\n",
    "    \"benchmark_together_ai/deepseek-ai/deepseek-llm-67b-chat_gpt-4o-2024-08-06_gemini-gemini-2.5-flash-preview-05-20_haicosystem_trial2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"gpt-4-turbo\",\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"together_ai/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\",\n",
    "    \"together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "]\n",
    "\n",
    "tags = [\n",
    "    \"benchmark_gpt-4-turbo_gemini/gemini-2.5-flash-preview-05-20_gpt-4o-2024-08-06_haicosystem_gemini_partner\",\n",
    "    \"benchmark_gpt-3.5-turbo_gemini/gemini-2.5-flash-preview-05-20_gpt-4o-2024-08-06_haicosystem_gemini_partner\",\n",
    "    \"benchmark_together_ai/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo_gemini/gemini-2.5-flash-preview-05-20_gpt-4o-2024-08-06_haicosystem_gemini_partner\",\n",
    "    \"benchmark_together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo_gemini/gemini-2.5-flash-preview-05-20_gpt-4o-2024-08-06_haicosystem_gemini_partner\",\n",
    "]\n",
    "\n",
    "models_mapping = {\n",
    "    \"gpt-4-turbo\": [\"GPT-4-turbo\", \"OpenAI\", \"No\"],\n",
    "    \"gpt-3.5-turbo\": [\"GPT-3.5-turbo\", \"OpenAI\", \"No\"],\n",
    "    \"together_ai/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\": [\n",
    "        \"Llama3.1-405B\",\n",
    "        \"Meta\",\n",
    "        \"Yes\",\n",
    "    ],\n",
    "    \"together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\": [\n",
    "        \"Llama3.1-8B\",\n",
    "        \"Meta\",\n",
    "        \"Yes\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"o1-2024-12-17\",\n",
    "    \"together_ai/deepseek-ai/DeepSeek-R1\",\n",
    "]\n",
    "\n",
    "tags = [\n",
    "    \"benchmark_o1-2024-12-17_gpt-4o-2024-08-06_gpt-4o-2024-08-06_haicosystem_trial2\",\n",
    "    \"benchmark_together_ai/deepseek-ai/DeepSeek-R1_gpt-4o-2024-08-06_gpt-4o-2024-08-06_haicosystem_trial2\",\n",
    "]\n",
    "\n",
    "models_mapping = {\n",
    "    \"o1-2024-12-17\": [\"O1\", \"OpenAI\", \"No\"],\n",
    "    \"together_ai/deepseek-ai/DeepSeek-R1\": [\"R1\", \"DeepSeek AI\", \"Yes\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get performance and ratio data\n",
    "from sotopia.database import EpisodeLog\n",
    "\n",
    "from haicosystem.protocols import HaiEnvironmentProfile\n",
    "from haicosystem.utils import get_avg_reward\n",
    "\n",
    "performance_data = {}\n",
    "ratio_data = {}\n",
    "model_rewards = {}\n",
    "all_episodes = []\n",
    "for model, tag in zip(models, tags):\n",
    "    episodes = EpisodeLog.find(EpisodeLog.tag == tag).all()\n",
    "    # Filter out episodes with invalid rewards structure\n",
    "    episodes = [\n",
    "        ep\n",
    "        for ep in episodes\n",
    "        if (\n",
    "            hasattr(ep, \"rewards\")\n",
    "            and isinstance(ep.rewards, list)\n",
    "            and len(ep.rewards) == 2\n",
    "            and not isinstance(ep.rewards[0], float)\n",
    "            and not isinstance(ep.rewards[1], float)\n",
    "        )\n",
    "    ]\n",
    "    all_episodes.extend(episodes)\n",
    "    print(f\"model: {model},  episodes: {len(episodes)}, tag: {tag}\")\n",
    "    avg_rewards = get_avg_reward(episodes, model)  # type: ignore\n",
    "    binary_avg_rewards = get_avg_reward(episodes, model, binary=True)  # type: ignore\n",
    "    performance_data[model] = avg_rewards\n",
    "    ratio_data[model] = binary_avg_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce the markdown table given the ratio data\n",
    "table_header = \"| Model Name | Publisher | Open? | Overall Risk Ratio | Targeted Safety Risks | System and Operational Risks | Content Safety Risks | Societal Risks | Legal and Rights Related Risks |\\n\"\n",
    "table_divider = \"| --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n\"\n",
    "\n",
    "dimensions = {\n",
    "    \"targeted_safety_risks\": \"Targeted Safety Risks & [-10, 0]\",\n",
    "    \"system_and_operational_risks\": \"System and Operational Risks & [-10, 0]\",\n",
    "    \"content_safety_risks\": \"Content Safety Risks & [-10, 0]\",\n",
    "    \"societal_risks\": \"Societal Risks & [-10, 0]\",\n",
    "    \"legal_and_rights_related_risks\": \"Legal and Rights-Related Risks & [-10, 0]\",\n",
    "    \"efficiency\": \"Efficiency & [0, 10]\",\n",
    "    \"goal\": \"Goal Completion & [0, 10]\",\n",
    "    \"overall_score\": \"Overall Score & [0, 10]\",\n",
    "}\n",
    "\n",
    "\n",
    "def process_scores(scores):\n",
    "    # return f\"{scores[0]} <sub>&plusmn; {scores[1]}</sub>\"\n",
    "    return f\"{scores[0]:.2f}\"\n",
    "\n",
    "\n",
    "table_rows = \"\"\n",
    "for model in models:\n",
    "    model_name = models_mapping[model][0]\n",
    "    publisher = models_mapping[model][1]\n",
    "    open_status = models_mapping[model][2]\n",
    "    overall_score = process_scores(ratio_data[model].get(\"overall_score\", \"N/A\"))\n",
    "    targeted_safety_risks = process_scores(\n",
    "        ratio_data[model].get(\"targeted_safety_risks\", \"N/A\")\n",
    "    )\n",
    "    system_operational_risks = process_scores(\n",
    "        ratio_data[model].get(\"system_and_operational_risks\", \"N/A\")\n",
    "    )\n",
    "    content_safety_risks = process_scores(\n",
    "        ratio_data[model].get(\"content_safety_risks\", \"N/A\")\n",
    "    )\n",
    "    societal_risks = process_scores(ratio_data[model].get(\"societal_risks\", \"N/A\"))\n",
    "    legal_rights_risks = process_scores(\n",
    "        ratio_data[model].get(\"legal_and_rights_related_risks\", \"N/A\")\n",
    "    )\n",
    "    # efficiency = process_scores(ratio_data[model].get('efficiency', 'N/A'))\n",
    "    # goal_completion = process_scores(ratio_data[model].get('goal', 'N/A'))\n",
    "\n",
    "    table_rows += f\"| {model_name} | {publisher} | {open_status} | {overall_score} | {targeted_safety_risks} | {system_operational_risks} | {content_safety_risks} | {societal_risks} | {legal_rights_risks} |\\n\"\n",
    "\n",
    "markdown_table = table_header + table_divider + table_rows\n",
    "\n",
    "# Display the markdown table\n",
    "markdown_table\n",
    "\n",
    "# Model Name | Publisher | Open? | Targeted Safety Risks | System and Operational Risks | Content Safety Risks | Societal Risks | Legal and Rights Related Risks | Efficiency | Goal Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk Scors\n",
    "table_header = \"| Model Name | Publisher | Open? | Overall Score | Targeted Safety Risks | System and Operational Risks | Content Safety Risks | Societal Risks | Legal and Rights Related Risks | Goal Completion | Efficiency |\\n\"\n",
    "table_divider = \"| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n\"\n",
    "\n",
    "table_rows = \"\"\n",
    "for model in models:\n",
    "    model_name = models_mapping[model][0]\n",
    "    publisher = models_mapping[model][1]\n",
    "    open_status = models_mapping[model][2]\n",
    "    targeted_safety_risks = process_scores(\n",
    "        performance_data[model].get(\"targeted_safety_risks\", \"N/A\")\n",
    "    )\n",
    "    system_operational_risks = process_scores(\n",
    "        performance_data[model].get(\"system_and_operational_risks\", \"N/A\")\n",
    "    )\n",
    "    content_safety_risks = process_scores(\n",
    "        performance_data[model].get(\"content_safety_risks\", \"N/A\")\n",
    "    )\n",
    "    societal_risks = process_scores(\n",
    "        performance_data[model].get(\"societal_risks\", \"N/A\")\n",
    "    )\n",
    "    legal_rights_risks = process_scores(\n",
    "        performance_data[model].get(\"legal_and_rights_related_risks\", \"N/A\")\n",
    "    )\n",
    "    goal_completion = process_scores(performance_data[model].get(\"goal\", \"N/A\"))\n",
    "    efficiency = process_scores(performance_data[model].get(\"efficiency\", \"N/A\"))\n",
    "    overall_score = (\n",
    "        sum(\n",
    "            [\n",
    "                float(targeted_safety_risks),\n",
    "                float(system_operational_risks),\n",
    "                float(content_safety_risks),\n",
    "                float(societal_risks),\n",
    "                float(legal_rights_risks),\n",
    "                float(goal_completion),\n",
    "                float(efficiency),\n",
    "            ]\n",
    "        )\n",
    "        / 7\n",
    "    )\n",
    "    overall_score = process_scores((overall_score, 0))\n",
    "\n",
    "    table_rows += f\"| {model_name} | {publisher} | {open_status} | {overall_score} | {targeted_safety_risks} | {system_operational_risks} | {content_safety_risks} | {societal_risks} | {legal_rights_risks} | {goal_completion} | {efficiency} |\\n\"\n",
    "\n",
    "markdown_table = table_header + table_divider + table_rows\n",
    "import subprocess\n",
    "\n",
    "data = markdown_table\n",
    "subprocess.run(\"pbcopy\", text=True, input=data)\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haicosystem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
